#!/bin/bash

# ========================================
# Script de Instala√ß√£o Autom√°tica
# Assistente IA - Ollama + LLaMA 3.1
# Para Linux/Mac
# ========================================

echo "========================================"
echo "  Instala√ß√£o do Assistente IA"
echo "  Ollama + LLaMA 3.1"
echo "========================================"
echo ""

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Passo 1: Instalar Ollama
echo -e "${GREEN}üì¶ Passo 1: Instalando Ollama...${NC}"
echo ""

if command -v ollama &> /dev/null; then
    echo -e "${GREEN}‚úÖ Ollama j√° est√° instalado!${NC}"
else
    echo -e "${YELLOW}Baixando e instalando Ollama...${NC}"
    curl -fsSL https://ollama.com/install.sh | sh
    
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}‚úÖ Ollama instalado com sucesso!${NC}"
    else
        echo -e "${RED}‚ùå Erro ao instalar Ollama${NC}"
        exit 1
    fi
fi

echo ""
sleep 2

# Passo 2: Iniciar servi√ßo
echo -e "${YELLOW}‚è≥ Iniciando servi√ßo Ollama...${NC}"
ollama serve &
OLLAMA_PID=$!
sleep 5

# Verificar se est√° rodando
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo -e "${GREEN}‚úÖ Servi√ßo Ollama est√° rodando!${NC}"
else
    echo -e "${RED}‚ùå Servi√ßo n√£o iniciou${NC}"
    exit 1
fi

echo ""
sleep 2

# Passo 3: Baixar modelo
echo -e "${GREEN}üì• Passo 2: Baixando modelo LLaMA 3.1 (8B)...${NC}"
echo -e "${YELLOW}‚ö†Ô∏è  Isso pode demorar alguns minutos (~4.7 GB)${NC}"
echo ""

if ollama list | grep -q "llama3.1:8b"; then
    echo -e "${GREEN}‚úÖ Modelo LLaMA 3.1:8b j√° est√° instalado!${NC}"
else
    echo -e "${YELLOW}Baixando modelo...${NC}"
    ollama pull llama3.1:8b
    
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}‚úÖ Modelo baixado com sucesso!${NC}"
    else
        echo -e "${RED}‚ùå Erro ao baixar modelo${NC}"
        exit 1
    fi
fi

echo ""
sleep 2

# Passo 4: Testar
echo -e "${GREEN}üß™ Passo 3: Testando modelo...${NC}"
echo ""

TEST_RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
    -H "Content-Type: application/json" \
    -d '{
        "model": "llama3.1:8b",
        "prompt": "Responda em portugu√™s: Ol√°, voc√™ est√° funcionando?",
        "stream": false
    }')

if [ $? -eq 0 ]; then
    echo -e "${GREEN}‚úÖ Modelo funcionando!${NC}"
    echo -e "${CYAN}Resposta: $(echo $TEST_RESPONSE | jq -r '.response')${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  N√£o foi poss√≠vel testar o modelo${NC}"
fi

echo ""
echo "========================================"
echo -e "${GREEN}  ‚úÖ INSTALA√á√ÉO CONCLU√çDA!${NC}"
echo "========================================"
echo ""

echo -e "${YELLOW}üìã Resumo:${NC}"
echo -e "${GREEN}  ‚úÖ Ollama instalado${NC}"
echo -e "${GREEN}  ‚úÖ Modelo LLaMA 3.1:8b baixado${NC}"
echo -e "${GREEN}  ‚úÖ Servi√ßo rodando em http://localhost:11434${NC}"
echo ""

echo -e "${YELLOW}üöÄ Pr√≥ximos Passos:${NC}"
echo "  1. Abra um novo terminal"
echo "  2. Navegue at√©: cd backend"
echo "  3. Inicie o backend: npm run dev"
echo "  4. Em outro terminal, inicie o frontend: npm run dev"
echo "  5. Fa√ßa login no sistema"
echo "  6. Clique no bot√£o ‚ú® no canto inferior direito"
echo "  7. Teste: 'Quanto vendi hoje?'"
echo ""

echo -e "${YELLOW}üí° Comandos √∫teis:${NC}"
echo -e "${CYAN}  ollama list              - Ver modelos instalados${NC}"
echo -e "${CYAN}  ollama serve             - Iniciar servi√ßo manualmente${NC}"
echo -e "${CYAN}  ollama run llama3.1:8b   - Testar modelo no terminal${NC}"
echo ""
